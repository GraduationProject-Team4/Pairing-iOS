// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/cloud/speech/v1p1beta1/cloud_speech.proto

// This CPP symbol can be defined to use imports that match up to the framework
// imports needed when using CocoaPods.
#if !defined(GPB_USE_PROTOBUF_FRAMEWORK_IMPORTS)
 #define GPB_USE_PROTOBUF_FRAMEWORK_IMPORTS 0
#endif

#if GPB_USE_PROTOBUF_FRAMEWORK_IMPORTS
 #import <Protobuf/GPBProtocolBuffers.h>
#else
 #import "GPBProtocolBuffers.h"
#endif

#if GOOGLE_PROTOBUF_OBJC_VERSION < 30004
#error This file was generated by a newer version of protoc which is incompatible with your Protocol Buffer library sources.
#endif
#if 30004 < GOOGLE_PROTOBUF_OBJC_MIN_SUPPORTED_VERSION
#error This file was generated by an older version of protoc which is incompatible with your Protocol Buffer library sources.
#endif

// @@protoc_insertion_point(imports)

#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdeprecated-declarations"

CF_EXTERN_C_BEGIN

@class RecognitionAudio;
@class RecognitionConfig;
@class RecognitionMetadata;
@class SpeechContext;
@class SpeechRecognitionAlternative;
@class SpeechRecognitionResult;
@class Status;
@class StreamingRecognitionConfig;
@class StreamingRecognitionResult;
@class WordInfo;

NS_ASSUME_NONNULL_BEGIN

#pragma mark - Enum RecognitionConfig_AudioEncoding

/**
 * The encoding of the audio data sent in the request.
 *
 * All encodings support only 1 channel (mono) audio.
 *
 * For best results, the audio source should be captured and transmitted using
 * a lossless encoding (`FLAC` or `LINEAR16`). The accuracy of the speech
 * recognition can be reduced if lossy codecs are used to capture or transmit
 * audio, particularly if background noise is present. Lossy codecs include
 * `MULAW`, `AMR`, `AMR_WB`, `OGG_OPUS`, and `SPEEX_WITH_HEADER_BYTE`.
 *
 * The `FLAC` and `WAV` audio file formats include a header that describes the
 * included audio content. You can request recognition for `WAV` files that
 * contain either `LINEAR16` or `MULAW` encoded audio.
 * If you send `FLAC` or `WAV` audio file format in
 * your request, you do not need to specify an `AudioEncoding`; the audio
 * encoding format is determined from the file header. If you specify
 * an `AudioEncoding` when you send  send `FLAC` or `WAV` audio, the
 * encoding configuration must match the encoding described in the audio
 * header; otherwise the request returns an
 * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] error
 * code.
 **/
typedef GPB_ENUM(RecognitionConfig_AudioEncoding) {
  /**
   * Value used if any message's field encounters a value that is not defined
   * by this enum. The message will also have C functions to get/set the rawValue
   * of the field.
   **/
  RecognitionConfig_AudioEncoding_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  /** Not specified. */
  RecognitionConfig_AudioEncoding_EncodingUnspecified = 0,

  /** Uncompressed 16-bit signed little-endian samples (Linear PCM). */
  RecognitionConfig_AudioEncoding_Linear16 = 1,

  /**
   * `FLAC` (Free Lossless Audio
   * Codec) is the recommended encoding because it is
   * lossless--therefore recognition is not compromised--and
   * requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
   * encoding supports 16-bit and 24-bit samples, however, not all fields in
   * `STREAMINFO` are supported.
   **/
  RecognitionConfig_AudioEncoding_Flac = 2,

  /** 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law. */
  RecognitionConfig_AudioEncoding_Mulaw = 3,

  /** Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000. */
  RecognitionConfig_AudioEncoding_Amr = 4,

  /** Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000. */
  RecognitionConfig_AudioEncoding_AmrWb = 5,

  /**
   * Opus encoded audio frames in Ogg container
   * ([OggOpus](https://wiki.xiph.org/OggOpus)).
   * `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000, or 48000.
   **/
  RecognitionConfig_AudioEncoding_OggOpus = 6,

  /**
   * Although the use of lossy encodings is not recommended, if a very low
   * bitrate encoding is required, `OGG_OPUS` is highly preferred over
   * Speex encoding. The [Speex](https://speex.org/)  encoding supported by
   * Cloud Speech API has a header byte in each block, as in MIME type
   * `audio/x-speex-with-header-byte`.
   * It is a variant of the RTP Speex encoding defined in
   * [RFC 5574](https://tools.ietf.org/html/rfc5574).
   * The stream is a sequence of blocks, one block per RTP packet. Each block
   * starts with a byte containing the length of the block, in bytes, followed
   * by one or more frames of Speex data, padded to an integral number of
   * bytes (octets) as specified in RFC 5574. In other words, each RTP header
   * is replaced with a single byte containing the block length. Only Speex
   * wideband is supported. `sample_rate_hertz` must be 16000.
   **/
  RecognitionConfig_AudioEncoding_SpeexWithHeaderByte = 7,
};

GPBEnumDescriptor *RecognitionConfig_AudioEncoding_EnumDescriptor(void);

/**
 * Checks to see if the given value is defined by the enum or was not known at
 * the time this source was generated.
 **/
BOOL RecognitionConfig_AudioEncoding_IsValidValue(int32_t value);

#pragma mark - Enum RecognitionMetadata_InteractionType

/**
 * Use case categories that the audio recognition request can be described
 * by.
 **/
typedef GPB_ENUM(RecognitionMetadata_InteractionType) {
  /**
   * Value used if any message's field encounters a value that is not defined
   * by this enum. The message will also have C functions to get/set the rawValue
   * of the field.
   **/
  RecognitionMetadata_InteractionType_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  /**
   * Use case is either unknown or is something other than one of the other
   * values below.
   **/
  RecognitionMetadata_InteractionType_InteractionTypeUnspecified = 0,

  /**
   * Multiple people in a conversation or discussion. For example in a
   * meeting with two or more people actively participating. Typically
   * all the primary people speaking would be in the same room (if not,
   * see PHONE_CALL)
   **/
  RecognitionMetadata_InteractionType_Discussion = 1,

  /**
   * One or more persons lecturing or presenting to others, mostly
   * uninterrupted.
   **/
  RecognitionMetadata_InteractionType_Presentation = 2,

  /**
   * A phone-call or video-conference in which two or more people, who are
   * not in the same room, are actively participating.
   **/
  RecognitionMetadata_InteractionType_PhoneCall = 3,

  /** A recorded message intended for another person to listen to. */
  RecognitionMetadata_InteractionType_Voicemail = 4,

  /** Professionally produced audio (eg. TV Show, Podcast). */
  RecognitionMetadata_InteractionType_ProfessionallyProduced = 5,

  /** Transcribe spoken questions and queries into text. */
  RecognitionMetadata_InteractionType_VoiceSearch = 6,

  /** Transcribe voice commands, such as for controlling a device. */
  RecognitionMetadata_InteractionType_VoiceCommand = 7,

  /**
   * Transcribe speech to text to create a written document, such as a
   * text-message, email or report.
   **/
  RecognitionMetadata_InteractionType_Dictation = 8,
};

GPBEnumDescriptor *RecognitionMetadata_InteractionType_EnumDescriptor(void);

/**
 * Checks to see if the given value is defined by the enum or was not known at
 * the time this source was generated.
 **/
BOOL RecognitionMetadata_InteractionType_IsValidValue(int32_t value);

#pragma mark - Enum RecognitionMetadata_MicrophoneDistance

/** Enumerates the types of capture settings describing an audio file. */
typedef GPB_ENUM(RecognitionMetadata_MicrophoneDistance) {
  /**
   * Value used if any message's field encounters a value that is not defined
   * by this enum. The message will also have C functions to get/set the rawValue
   * of the field.
   **/
  RecognitionMetadata_MicrophoneDistance_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  /** Audio type is not known. */
  RecognitionMetadata_MicrophoneDistance_MicrophoneDistanceUnspecified = 0,

  /**
   * The audio was captured from a closely placed microphone. Eg. phone,
   * dictaphone, or handheld microphone. Generally if there speaker is within
   * 1 meter of the microphone.
   **/
  RecognitionMetadata_MicrophoneDistance_Nearfield = 1,

  /** The speaker if within 3 meters of the microphone. */
  RecognitionMetadata_MicrophoneDistance_Midfield = 2,

  /** The speaker is more than 3 meters away from the microphone. */
  RecognitionMetadata_MicrophoneDistance_Farfield = 3,
};

GPBEnumDescriptor *RecognitionMetadata_MicrophoneDistance_EnumDescriptor(void);

/**
 * Checks to see if the given value is defined by the enum or was not known at
 * the time this source was generated.
 **/
BOOL RecognitionMetadata_MicrophoneDistance_IsValidValue(int32_t value);

#pragma mark - Enum RecognitionMetadata_OriginalMediaType

/** The original media the speech was recorded on. */
typedef GPB_ENUM(RecognitionMetadata_OriginalMediaType) {
  /**
   * Value used if any message's field encounters a value that is not defined
   * by this enum. The message will also have C functions to get/set the rawValue
   * of the field.
   **/
  RecognitionMetadata_OriginalMediaType_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  /** Unknown original media type. */
  RecognitionMetadata_OriginalMediaType_OriginalMediaTypeUnspecified = 0,

  /** The speech data is an audio recording. */
  RecognitionMetadata_OriginalMediaType_Audio = 1,

  /** The speech data originally recorded on a video. */
  RecognitionMetadata_OriginalMediaType_Video = 2,
};

GPBEnumDescriptor *RecognitionMetadata_OriginalMediaType_EnumDescriptor(void);

/**
 * Checks to see if the given value is defined by the enum or was not known at
 * the time this source was generated.
 **/
BOOL RecognitionMetadata_OriginalMediaType_IsValidValue(int32_t value);

#pragma mark - Enum RecognitionMetadata_RecordingDeviceType

/** The type of device the speech was recorded with. */
typedef GPB_ENUM(RecognitionMetadata_RecordingDeviceType) {
  /**
   * Value used if any message's field encounters a value that is not defined
   * by this enum. The message will also have C functions to get/set the rawValue
   * of the field.
   **/
  RecognitionMetadata_RecordingDeviceType_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  /** The recording device is unknown. */
  RecognitionMetadata_RecordingDeviceType_RecordingDeviceTypeUnspecified = 0,

  /** Speech was recorded on a smartphone. */
  RecognitionMetadata_RecordingDeviceType_Smartphone = 1,

  /** Speech was recorded using a personal computer or tablet. */
  RecognitionMetadata_RecordingDeviceType_Pc = 2,

  /** Speech was recorded over a phone line. */
  RecognitionMetadata_RecordingDeviceType_PhoneLine = 3,

  /** Speech was recorded in a vehicle. */
  RecognitionMetadata_RecordingDeviceType_Vehicle = 4,

  /** Speech was recorded outdoors. */
  RecognitionMetadata_RecordingDeviceType_OtherOutdoorDevice = 5,

  /** Speech was recorded indoors. */
  RecognitionMetadata_RecordingDeviceType_OtherIndoorDevice = 6,
};

GPBEnumDescriptor *RecognitionMetadata_RecordingDeviceType_EnumDescriptor(void);

/**
 * Checks to see if the given value is defined by the enum or was not known at
 * the time this source was generated.
 **/
BOOL RecognitionMetadata_RecordingDeviceType_IsValidValue(int32_t value);

#pragma mark - Enum StreamingRecognizeResponse_SpeechEventType

/** Indicates the type of speech event. */
typedef GPB_ENUM(StreamingRecognizeResponse_SpeechEventType) {
  /**
   * Value used if any message's field encounters a value that is not defined
   * by this enum. The message will also have C functions to get/set the rawValue
   * of the field.
   **/
  StreamingRecognizeResponse_SpeechEventType_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  /** No speech event specified. */
  StreamingRecognizeResponse_SpeechEventType_SpeechEventUnspecified = 0,

  /**
   * This event indicates that the server has detected the end of the user's
   * speech utterance and expects no additional speech. Therefore, the server
   * will not process additional audio (although it may subsequently return
   * additional results). The client should stop sending additional audio
   * data, half-close the gRPC connection, and wait for any additional results
   * until the server closes the gRPC connection. This event is only sent if
   * `single_utterance` was set to `true`, and is not used otherwise.
   **/
  StreamingRecognizeResponse_SpeechEventType_EndOfSingleUtterance = 1,
};

GPBEnumDescriptor *StreamingRecognizeResponse_SpeechEventType_EnumDescriptor(void);

/**
 * Checks to see if the given value is defined by the enum or was not known at
 * the time this source was generated.
 **/
BOOL StreamingRecognizeResponse_SpeechEventType_IsValidValue(int32_t value);

#pragma mark - CloudSpeechRoot

/**
 * Exposes the extension registry for this file.
 *
 * The base class provides:
 * @code
 *   + (GPBExtensionRegistry *)extensionRegistry;
 * @endcode
 * which is a @c GPBExtensionRegistry that includes all the extensions defined by
 * this file and all files that it depends on.
 **/
GPB_FINAL @interface CloudSpeechRoot : GPBRootObject
@end

#pragma mark - RecognizeRequest

typedef GPB_ENUM(RecognizeRequest_FieldNumber) {
  RecognizeRequest_FieldNumber_Config = 1,
  RecognizeRequest_FieldNumber_Audio = 2,
};

/**
 * The top-level message sent by the client for the `Recognize` method.
 **/
GPB_FINAL @interface RecognizeRequest : GPBMessage

/**
 * *Required* Provides information to the recognizer that specifies how to
 * process the request.
 **/
@property(nonatomic, readwrite, strong, null_resettable) RecognitionConfig *config;
/** Test to see if @c config has been set. */
@property(nonatomic, readwrite) BOOL hasConfig;

/** *Required* The audio data to be recognized. */
@property(nonatomic, readwrite, strong, null_resettable) RecognitionAudio *audio;
/** Test to see if @c audio has been set. */
@property(nonatomic, readwrite) BOOL hasAudio;

@end

#pragma mark - LongRunningRecognizeRequest

typedef GPB_ENUM(LongRunningRecognizeRequest_FieldNumber) {
  LongRunningRecognizeRequest_FieldNumber_Config = 1,
  LongRunningRecognizeRequest_FieldNumber_Audio = 2,
};

/**
 * The top-level message sent by the client for the `LongRunningRecognize`
 * method.
 **/
GPB_FINAL @interface LongRunningRecognizeRequest : GPBMessage

/**
 * *Required* Provides information to the recognizer that specifies how to
 * process the request.
 **/
@property(nonatomic, readwrite, strong, null_resettable) RecognitionConfig *config;
/** Test to see if @c config has been set. */
@property(nonatomic, readwrite) BOOL hasConfig;

/** *Required* The audio data to be recognized. */
@property(nonatomic, readwrite, strong, null_resettable) RecognitionAudio *audio;
/** Test to see if @c audio has been set. */
@property(nonatomic, readwrite) BOOL hasAudio;

@end

#pragma mark - StreamingRecognizeRequest

typedef GPB_ENUM(StreamingRecognizeRequest_FieldNumber) {
  StreamingRecognizeRequest_FieldNumber_StreamingConfig = 1,
  StreamingRecognizeRequest_FieldNumber_AudioContent = 2,
};

typedef GPB_ENUM(StreamingRecognizeRequest_StreamingRequest_OneOfCase) {
  StreamingRecognizeRequest_StreamingRequest_OneOfCase_GPBUnsetOneOfCase = 0,
  StreamingRecognizeRequest_StreamingRequest_OneOfCase_StreamingConfig = 1,
  StreamingRecognizeRequest_StreamingRequest_OneOfCase_AudioContent = 2,
};

/**
 * The top-level message sent by the client for the `StreamingRecognize` method.
 * Multiple `StreamingRecognizeRequest` messages are sent. The first message
 * must contain a `streaming_config` message and must not contain `audio` data.
 * All subsequent messages must contain `audio` data and must not contain a
 * `streaming_config` message.
 **/
GPB_FINAL @interface StreamingRecognizeRequest : GPBMessage

/** The streaming request, which is either a streaming config or audio content. */
@property(nonatomic, readonly) StreamingRecognizeRequest_StreamingRequest_OneOfCase streamingRequestOneOfCase;

/**
 * Provides information to the recognizer that specifies how to process the
 * request. The first `StreamingRecognizeRequest` message must contain a
 * `streaming_config`  message.
 **/
@property(nonatomic, readwrite, strong, null_resettable) StreamingRecognitionConfig *streamingConfig;

/**
 * The audio data to be recognized. Sequential chunks of audio data are sent
 * in sequential `StreamingRecognizeRequest` messages. The first
 * `StreamingRecognizeRequest` message must not contain `audio_content` data
 * and all subsequent `StreamingRecognizeRequest` messages must contain
 * `audio_content` data. The audio bytes must be encoded as specified in
 * `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
 * pure binary representation (not base64). See
 * [content limits](/speech-to-text/quotas#content).
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSData *audioContent;

@end

/**
 * Clears whatever value was set for the oneof 'streamingRequest'.
 **/
void StreamingRecognizeRequest_ClearStreamingRequestOneOfCase(StreamingRecognizeRequest *message);

#pragma mark - StreamingRecognitionConfig

typedef GPB_ENUM(StreamingRecognitionConfig_FieldNumber) {
  StreamingRecognitionConfig_FieldNumber_Config = 1,
  StreamingRecognitionConfig_FieldNumber_SingleUtterance = 2,
  StreamingRecognitionConfig_FieldNumber_InterimResults = 3,
};

/**
 * Provides information to the recognizer that specifies how to process the
 * request.
 **/
GPB_FINAL @interface StreamingRecognitionConfig : GPBMessage

/**
 * *Required* Provides information to the recognizer that specifies how to
 * process the request.
 **/
@property(nonatomic, readwrite, strong, null_resettable) RecognitionConfig *config;
/** Test to see if @c config has been set. */
@property(nonatomic, readwrite) BOOL hasConfig;

/**
 * *Optional* If `false` or omitted, the recognizer will perform continuous
 * recognition (continuing to wait for and process audio even if the user
 * pauses speaking) until the client closes the input stream (gRPC API) or
 * until the maximum time limit has been reached. May return multiple
 * `StreamingRecognitionResult`s with the `is_final` flag set to `true`.
 *
 * If `true`, the recognizer will detect a single spoken utterance. When it
 * detects that the user has paused or stopped speaking, it will return an
 * `END_OF_SINGLE_UTTERANCE` event and cease recognition. It will return no
 * more than one `StreamingRecognitionResult` with the `is_final` flag set to
 * `true`.
 **/
@property(nonatomic, readwrite) BOOL singleUtterance;

/**
 * *Optional* If `true`, interim results (tentative hypotheses) may be
 * returned as they become available (these interim results are indicated with
 * the `is_final=false` flag).
 * If `false` or omitted, only `is_final=true` result(s) are returned.
 **/
@property(nonatomic, readwrite) BOOL interimResults;

@end

#pragma mark - RecognitionConfig

typedef GPB_ENUM(RecognitionConfig_FieldNumber) {
  RecognitionConfig_FieldNumber_Encoding = 1,
  RecognitionConfig_FieldNumber_SampleRateHertz = 2,
  RecognitionConfig_FieldNumber_LanguageCode = 3,
  RecognitionConfig_FieldNumber_MaxAlternatives = 4,
  RecognitionConfig_FieldNumber_ProfanityFilter = 5,
  RecognitionConfig_FieldNumber_SpeechContextsArray = 6,
  RecognitionConfig_FieldNumber_AudioChannelCount = 7,
  RecognitionConfig_FieldNumber_EnableWordTimeOffsets = 8,
  RecognitionConfig_FieldNumber_Metadata = 9,
  RecognitionConfig_FieldNumber_EnableAutomaticPunctuation = 11,
  RecognitionConfig_FieldNumber_EnableSeparateRecognitionPerChannel = 12,
  RecognitionConfig_FieldNumber_Model = 13,
  RecognitionConfig_FieldNumber_UseEnhanced = 14,
  RecognitionConfig_FieldNumber_EnableWordConfidence = 15,
  RecognitionConfig_FieldNumber_EnableSpeakerDiarization = 16,
  RecognitionConfig_FieldNumber_DiarizationSpeakerCount = 17,
  RecognitionConfig_FieldNumber_AlternativeLanguageCodesArray = 18,
};

/**
 * Provides information to the recognizer that specifies how to process the
 * request.
 **/
GPB_FINAL @interface RecognitionConfig : GPBMessage

/**
 * Encoding of audio data sent in all `RecognitionAudio` messages.
 * This field is optional for `FLAC` and `WAV` audio files and required
 * for all other audio formats. For details, see
 * [AudioEncoding][google.cloud.speech.v1p1beta1.RecognitionConfig.AudioEncoding].
 **/
@property(nonatomic, readwrite) RecognitionConfig_AudioEncoding encoding;

/**
 * Sample rate in Hertz of the audio data sent in all
 * `RecognitionAudio` messages. Valid values are: 8000-48000.
 * 16000 is optimal. For best results, set the sampling rate of the audio
 * source to 16000 Hz. If that's not possible, use the native sample rate of
 * the audio source (instead of re-sampling).
 * This field is optional for `FLAC` and `WAV` audio files and required
 * for all other audio formats. For details, see
 * [AudioEncoding][google.cloud.speech.v1p1beta1.RecognitionConfig.AudioEncoding].
 **/
@property(nonatomic, readwrite) int32_t sampleRateHertz;

/**
 * *Optional* The number of channels in the input audio data.
 * ONLY set this for MULTI-CHANNEL recognition.
 * Valid values for LINEAR16 and FLAC are `1`-`8`.
 * Valid values for OGG_OPUS are '1'-'254'.
 * Valid value for MULAW, AMR, AMR_WB and SPEEX_WITH_HEADER_BYTE is only `1`.
 * If `0` or omitted, defaults to one channel (mono).
 * Note: We only recognize the first channel by default.
 * To perform independent recognition on each channel set
 * `enable_separate_recognition_per_channel` to 'true'.
 **/
@property(nonatomic, readwrite) int32_t audioChannelCount;

/**
 * This needs to be set to ‘true’ explicitly and `audio_channel_count` > 1
 * to get each channel recognized separately. The recognition result will
 * contain a `channel_tag` field to state which channel that result belongs
 * to. If this is not true, we will only recognize the first channel. The
 * request is billed cumulatively for all channels recognized:
 * `audio_channel_count` multiplied by the length of the audio.
 **/
@property(nonatomic, readwrite) BOOL enableSeparateRecognitionPerChannel;

/**
 * *Required* The language of the supplied audio as a
 * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
 * Example: "en-US".
 * See [Language Support](/speech-to-text/docs/languages)
 * for a list of the currently supported language codes.
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *languageCode;

/**
 * *Optional* A list of up to 3 additional
 * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tags,
 * listing possible alternative languages of the supplied audio.
 * See [Language Support](/speech-to-text/docs/languages)
 * for a list of the currently supported language codes.
 * If alternative languages are listed, recognition result will contain
 * recognition in the most likely language detected including the main
 * language_code. The recognition result will include the language tag
 * of the language detected in the audio.
 * Note: This feature is only supported for Voice Command and Voice Search
 * use cases and performance may vary for other use cases (e.g., phone call
 * transcription).
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<NSString*> *alternativeLanguageCodesArray;
/** The number of items in @c alternativeLanguageCodesArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger alternativeLanguageCodesArray_Count;

/**
 * *Optional* Maximum number of recognition hypotheses to be returned.
 * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
 * within each `SpeechRecognitionResult`.
 * The server may return fewer than `max_alternatives`.
 * Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
 * one. If omitted, will return a maximum of one.
 **/
@property(nonatomic, readwrite) int32_t maxAlternatives;

/**
 * *Optional* If set to `true`, the server will attempt to filter out
 * profanities, replacing all but the initial character in each filtered word
 * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
 * won't be filtered out.
 **/
@property(nonatomic, readwrite) BOOL profanityFilter;

/**
 * *Optional* array of
 * [SpeechContext][google.cloud.speech.v1p1beta1.SpeechContext]. A means to
 * provide context to assist the speech recognition. For more information, see
 * [Phrase Hints](/speech-to-text/docs/basics#phrase-hints).
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<SpeechContext*> *speechContextsArray;
/** The number of items in @c speechContextsArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger speechContextsArray_Count;

/**
 * *Optional* If `true`, the top result includes a list of words and
 * the start and end time offsets (timestamps) for those words. If
 * `false`, no word-level time offset information is returned. The default is
 * `false`.
 **/
@property(nonatomic, readwrite) BOOL enableWordTimeOffsets;

/**
 * *Optional* If `true`, the top result includes a list of words and the
 * confidence for those words. If `false`, no word-level confidence
 * information is returned. The default is `false`.
 **/
@property(nonatomic, readwrite) BOOL enableWordConfidence;

/**
 * *Optional* If 'true', adds punctuation to recognition result hypotheses.
 * This feature is only available in select languages. Setting this for
 * requests in other languages has no effect at all.
 * The default 'false' value does not add punctuation to result hypotheses.
 * Note: This is currently offered as an experimental service, complimentary
 * to all users. In the future this may be exclusively available as a
 * premium feature.
 **/
@property(nonatomic, readwrite) BOOL enableAutomaticPunctuation;

/**
 * *Optional* If 'true', enables speaker detection for each recognized word in
 * the top alternative of the recognition result using a speaker_tag provided
 * in the WordInfo.
 * Note: When this is true, we send all the words from the beginning of the
 * audio for the top alternative in every consecutive STREAMING responses.
 * This is done in order to improve our speaker tags as our models learn to
 * identify the speakers in the conversation over time.
 * For non-streaming requests, the diarization results will be provided only
 * in the top alternative of the FINAL SpeechRecognitionResult.
 **/
@property(nonatomic, readwrite) BOOL enableSpeakerDiarization;

/**
 * *Optional*
 * If set, specifies the estimated number of speakers in the conversation.
 * If not set, defaults to '2'.
 * Ignored unless enable_speaker_diarization is set to true."
 **/
@property(nonatomic, readwrite) int32_t diarizationSpeakerCount;

/** *Optional* Metadata regarding this request. */
@property(nonatomic, readwrite, strong, null_resettable) RecognitionMetadata *metadata;
/** Test to see if @c metadata has been set. */
@property(nonatomic, readwrite) BOOL hasMetadata;

/**
 * *Optional* Which model to select for the given request. Select the model
 * best suited to your domain to get best results. If a model is not
 * explicitly specified, then we auto-select a model based on the parameters
 * in the RecognitionConfig.
 * <table>
 *   <tr>
 *     <td><b>Model</b></td>
 *     <td><b>Description</b></td>
 *   </tr>
 *   <tr>
 *     <td><code>command_and_search</code></td>
 *     <td>Best for short queries such as voice commands or voice search.</td>
 *   </tr>
 *   <tr>
 *     <td><code>phone_call</code></td>
 *     <td>Best for audio that originated from a phone call (typically
 *     recorded at an 8khz sampling rate).</td>
 *   </tr>
 *   <tr>
 *     <td><code>video</code></td>
 *     <td>Best for audio that originated from from video or includes multiple
 *         speakers. Ideally the audio is recorded at a 16khz or greater
 *         sampling rate. This is a premium model that costs more than the
 *         standard rate.</td>
 *   </tr>
 *   <tr>
 *     <td><code>default</code></td>
 *     <td>Best for audio that is not one of the specific audio models.
 *         For example, long-form audio. Ideally the audio is high-fidelity,
 *         recorded at a 16khz or greater sampling rate.</td>
 *   </tr>
 * </table>
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *model;

/**
 * *Optional* Set to true to use an enhanced model for speech recognition.
 * If `use_enhanced` is set to true and the `model` field is not set, then
 * an appropriate enhanced model is chosen if:
 * 1. project is eligible for requesting enhanced models
 * 2. an enhanced model exists for the audio
 *
 * If `use_enhanced` is true and an enhanced version of the specified model
 * does not exist, then the speech is recognized using the standard version
 * of the specified model.
 *
 * Enhanced speech models require that you opt-in to data logging using
 * instructions in the
 * [documentation](/speech-to-text/docs/enable-data-logging). If you set
 * `use_enhanced` to true and you have not enabled audio logging, then you
 * will receive an error.
 **/
@property(nonatomic, readwrite) BOOL useEnhanced;

@end

/**
 * Fetches the raw value of a @c RecognitionConfig's @c encoding property, even
 * if the value was not defined by the enum at the time the code was generated.
 **/
int32_t RecognitionConfig_Encoding_RawValue(RecognitionConfig *message);
/**
 * Sets the raw value of an @c RecognitionConfig's @c encoding property, allowing
 * it to be set to a value that was not defined by the enum at the time the code
 * was generated.
 **/
void SetRecognitionConfig_Encoding_RawValue(RecognitionConfig *message, int32_t value);

#pragma mark - RecognitionMetadata

typedef GPB_ENUM(RecognitionMetadata_FieldNumber) {
  RecognitionMetadata_FieldNumber_InteractionType = 1,
  RecognitionMetadata_FieldNumber_IndustryNaicsCodeOfAudio = 3,
  RecognitionMetadata_FieldNumber_MicrophoneDistance = 4,
  RecognitionMetadata_FieldNumber_OriginalMediaType = 5,
  RecognitionMetadata_FieldNumber_RecordingDeviceType = 6,
  RecognitionMetadata_FieldNumber_RecordingDeviceName = 7,
  RecognitionMetadata_FieldNumber_OriginalMimeType = 8,
  RecognitionMetadata_FieldNumber_ObfuscatedId = 9,
  RecognitionMetadata_FieldNumber_AudioTopic = 10,
};

/**
 * Description of audio data to be recognized.
 **/
GPB_FINAL @interface RecognitionMetadata : GPBMessage

/** The use case most closely describing the audio content to be recognized. */
@property(nonatomic, readwrite) RecognitionMetadata_InteractionType interactionType;

/**
 * The industry vertical to which this speech recognition request most
 * closely applies. This is most indicative of the topics contained
 * in the audio.  Use the 6-digit NAICS code to identify the industry
 * vertical - see https://www.naics.com/search/.
 **/
@property(nonatomic, readwrite) uint32_t industryNaicsCodeOfAudio;

/** The audio type that most closely describes the audio being recognized. */
@property(nonatomic, readwrite) RecognitionMetadata_MicrophoneDistance microphoneDistance;

/** The original media the speech was recorded on. */
@property(nonatomic, readwrite) RecognitionMetadata_OriginalMediaType originalMediaType;

/** The type of device the speech was recorded with. */
@property(nonatomic, readwrite) RecognitionMetadata_RecordingDeviceType recordingDeviceType;

/**
 * The device used to make the recording.  Examples 'Nexus 5X' or
 * 'Polycom SoundStation IP 6000' or 'POTS' or 'VoIP' or
 * 'Cardioid Microphone'.
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *recordingDeviceName;

/**
 * Mime type of the original audio file.  For example `audio/m4a`,
 * `audio/x-alaw-basic`, `audio/mp3`, `audio/3gpp`.
 * A list of possible audio mime types is maintained at
 * http://www.iana.org/assignments/media-types/media-types.xhtml#audio
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *originalMimeType;

/**
 * Obfuscated (privacy-protected) ID of the user, to identify number of
 * unique users using the service.
 **/
@property(nonatomic, readwrite) int64_t obfuscatedId;

/**
 * Description of the content. Eg. "Recordings of federal supreme court
 * hearings from 2012".
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *audioTopic;

@end

/**
 * Fetches the raw value of a @c RecognitionMetadata's @c interactionType property, even
 * if the value was not defined by the enum at the time the code was generated.
 **/
int32_t RecognitionMetadata_InteractionType_RawValue(RecognitionMetadata *message);
/**
 * Sets the raw value of an @c RecognitionMetadata's @c interactionType property, allowing
 * it to be set to a value that was not defined by the enum at the time the code
 * was generated.
 **/
void SetRecognitionMetadata_InteractionType_RawValue(RecognitionMetadata *message, int32_t value);

/**
 * Fetches the raw value of a @c RecognitionMetadata's @c microphoneDistance property, even
 * if the value was not defined by the enum at the time the code was generated.
 **/
int32_t RecognitionMetadata_MicrophoneDistance_RawValue(RecognitionMetadata *message);
/**
 * Sets the raw value of an @c RecognitionMetadata's @c microphoneDistance property, allowing
 * it to be set to a value that was not defined by the enum at the time the code
 * was generated.
 **/
void SetRecognitionMetadata_MicrophoneDistance_RawValue(RecognitionMetadata *message, int32_t value);

/**
 * Fetches the raw value of a @c RecognitionMetadata's @c originalMediaType property, even
 * if the value was not defined by the enum at the time the code was generated.
 **/
int32_t RecognitionMetadata_OriginalMediaType_RawValue(RecognitionMetadata *message);
/**
 * Sets the raw value of an @c RecognitionMetadata's @c originalMediaType property, allowing
 * it to be set to a value that was not defined by the enum at the time the code
 * was generated.
 **/
void SetRecognitionMetadata_OriginalMediaType_RawValue(RecognitionMetadata *message, int32_t value);

/**
 * Fetches the raw value of a @c RecognitionMetadata's @c recordingDeviceType property, even
 * if the value was not defined by the enum at the time the code was generated.
 **/
int32_t RecognitionMetadata_RecordingDeviceType_RawValue(RecognitionMetadata *message);
/**
 * Sets the raw value of an @c RecognitionMetadata's @c recordingDeviceType property, allowing
 * it to be set to a value that was not defined by the enum at the time the code
 * was generated.
 **/
void SetRecognitionMetadata_RecordingDeviceType_RawValue(RecognitionMetadata *message, int32_t value);

#pragma mark - SpeechContext

typedef GPB_ENUM(SpeechContext_FieldNumber) {
  SpeechContext_FieldNumber_PhrasesArray = 1,
};

/**
 * Provides "hints" to the speech recognizer to favor specific words and phrases
 * in the results.
 **/
GPB_FINAL @interface SpeechContext : GPBMessage

/**
 * *Optional* A list of strings containing words and phrases "hints" so that
 * the speech recognition is more likely to recognize them. This can be used
 * to improve the accuracy for specific words and phrases, for example, if
 * specific commands are typically spoken by the user. This can also be used
 * to add additional words to the vocabulary of the recognizer. See
 * [usage limits](/speech-to-text/quotas#content).
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<NSString*> *phrasesArray;
/** The number of items in @c phrasesArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger phrasesArray_Count;

@end

#pragma mark - RecognitionAudio

typedef GPB_ENUM(RecognitionAudio_FieldNumber) {
  RecognitionAudio_FieldNumber_Content = 1,
  RecognitionAudio_FieldNumber_Uri = 2,
};

typedef GPB_ENUM(RecognitionAudio_AudioSource_OneOfCase) {
  RecognitionAudio_AudioSource_OneOfCase_GPBUnsetOneOfCase = 0,
  RecognitionAudio_AudioSource_OneOfCase_Content = 1,
  RecognitionAudio_AudioSource_OneOfCase_Uri = 2,
};

/**
 * Contains audio data in the encoding specified in the `RecognitionConfig`.
 * Either `content` or `uri` must be supplied. Supplying both or neither
 * returns [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
 * See [content limits](/speech-to-text/quotas#content).
 **/
GPB_FINAL @interface RecognitionAudio : GPBMessage

/**
 * The audio source, which is either inline content or a Google Cloud
 * Storage uri.
 **/
@property(nonatomic, readonly) RecognitionAudio_AudioSource_OneOfCase audioSourceOneOfCase;

/**
 * The audio data bytes encoded as specified in
 * `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
 * pure binary representation, whereas JSON representations use base64.
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSData *content;

/**
 * URI that points to a file that contains audio data bytes as specified in
 * `RecognitionConfig`. The file must not be compressed (for example, gzip).
 * Currently, only Google Cloud Storage URIs are
 * supported, which must be specified in the following format:
 * `gs://bucket_name/object_name` (other URI formats return
 * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]).
 * For more information, see [Request
 * URIs](https://cloud.google.com/storage/docs/reference-uris).
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *uri;

@end

/**
 * Clears whatever value was set for the oneof 'audioSource'.
 **/
void RecognitionAudio_ClearAudioSourceOneOfCase(RecognitionAudio *message);

#pragma mark - RecognizeResponse

typedef GPB_ENUM(RecognizeResponse_FieldNumber) {
  RecognizeResponse_FieldNumber_ResultsArray = 2,
};

/**
 * The only message returned to the client by the `Recognize` method. It
 * contains the result as zero or more sequential `SpeechRecognitionResult`
 * messages.
 **/
GPB_FINAL @interface RecognizeResponse : GPBMessage

/**
 * Output only. Sequential list of transcription results corresponding to
 * sequential portions of audio.
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<SpeechRecognitionResult*> *resultsArray;
/** The number of items in @c resultsArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger resultsArray_Count;

@end

#pragma mark - LongRunningRecognizeResponse

typedef GPB_ENUM(LongRunningRecognizeResponse_FieldNumber) {
  LongRunningRecognizeResponse_FieldNumber_ResultsArray = 2,
};

/**
 * The only message returned to the client by the `LongRunningRecognize` method.
 * It contains the result as zero or more sequential `SpeechRecognitionResult`
 * messages. It is included in the `result.response` field of the `Operation`
 * returned by the `GetOperation` call of the `google::longrunning::Operations`
 * service.
 **/
GPB_FINAL @interface LongRunningRecognizeResponse : GPBMessage

/**
 * Output only. Sequential list of transcription results corresponding to
 * sequential portions of audio.
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<SpeechRecognitionResult*> *resultsArray;
/** The number of items in @c resultsArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger resultsArray_Count;

@end

#pragma mark - LongRunningRecognizeMetadata

typedef GPB_ENUM(LongRunningRecognizeMetadata_FieldNumber) {
  LongRunningRecognizeMetadata_FieldNumber_ProgressPercent = 1,
  LongRunningRecognizeMetadata_FieldNumber_StartTime = 2,
  LongRunningRecognizeMetadata_FieldNumber_LastUpdateTime = 3,
};

/**
 * Describes the progress of a long-running `LongRunningRecognize` call. It is
 * included in the `metadata` field of the `Operation` returned by the
 * `GetOperation` call of the `google::longrunning::Operations` service.
 **/
GPB_FINAL @interface LongRunningRecognizeMetadata : GPBMessage

/**
 * Approximate percentage of audio processed thus far. Guaranteed to be 100
 * when the audio is fully processed and the results are available.
 **/
@property(nonatomic, readwrite) int32_t progressPercent;

/** Time when the request was received. */
@property(nonatomic, readwrite, strong, null_resettable) GPBTimestamp *startTime;
/** Test to see if @c startTime has been set. */
@property(nonatomic, readwrite) BOOL hasStartTime;

/** Time of the most recent processing update. */
@property(nonatomic, readwrite, strong, null_resettable) GPBTimestamp *lastUpdateTime;
/** Test to see if @c lastUpdateTime has been set. */
@property(nonatomic, readwrite) BOOL hasLastUpdateTime;

@end

#pragma mark - StreamingRecognizeResponse

typedef GPB_ENUM(StreamingRecognizeResponse_FieldNumber) {
  StreamingRecognizeResponse_FieldNumber_Error = 1,
  StreamingRecognizeResponse_FieldNumber_ResultsArray = 2,
  StreamingRecognizeResponse_FieldNumber_SpeechEventType = 4,
};

/**
 * `StreamingRecognizeResponse` is the only message returned to the client by
 * `StreamingRecognize`. A series of zero or more `StreamingRecognizeResponse`
 * messages are streamed back to the client. If there is no recognizable
 * audio, and `single_utterance` is set to false, then no messages are streamed
 * back to the client.
 *
 * Here's an example of a series of ten `StreamingRecognizeResponse`s that might
 * be returned while processing audio:
 *
 * 1. results { alternatives { transcript: "tube" } stability: 0.01 }
 *
 * 2. results { alternatives { transcript: "to be a" } stability: 0.01 }
 *
 * 3. results { alternatives { transcript: "to be" } stability: 0.9 }
 *    results { alternatives { transcript: " or not to be" } stability: 0.01 }
 *
 * 4. results { alternatives { transcript: "to be or not to be"
 *                             confidence: 0.92 }
 *              alternatives { transcript: "to bee or not to bee" }
 *              is_final: true }
 *
 * 5. results { alternatives { transcript: " that's" } stability: 0.01 }
 *
 * 6. results { alternatives { transcript: " that is" } stability: 0.9 }
 *    results { alternatives { transcript: " the question" } stability: 0.01 }
 *
 * 7. results { alternatives { transcript: " that is the question"
 *                             confidence: 0.98 }
 *              alternatives { transcript: " that was the question" }
 *              is_final: true }
 *
 * Notes:
 *
 * - Only two of the above responses #4 and #7 contain final results; they are
 *   indicated by `is_final: true`. Concatenating these together generates the
 *   full transcript: "to be or not to be that is the question".
 *
 * - The others contain interim `results`. #3 and #6 contain two interim
 *   `results`: the first portion has a high stability and is less likely to
 *   change; the second portion has a low stability and is very likely to
 *   change. A UI designer might choose to show only high stability `results`.
 *
 * - The specific `stability` and `confidence` values shown above are only for
 *   illustrative purposes. Actual values may vary.
 *
 * - In each response, only one of these fields will be set:
 *     `error`,
 *     `speech_event_type`, or
 *     one or more (repeated) `results`.
 **/
GPB_FINAL @interface StreamingRecognizeResponse : GPBMessage

/**
 * Output only. If set, returns a [google.rpc.Status][google.rpc.Status]
 * message that specifies the error for the operation.
 **/
@property(nonatomic, readwrite, strong, null_resettable) Status *error;
/** Test to see if @c error has been set. */
@property(nonatomic, readwrite) BOOL hasError;

/**
 * Output only. This repeated list contains zero or more results that
 * correspond to consecutive portions of the audio currently being processed.
 * It contains zero or one `is_final=true` result (the newly settled portion),
 * followed by zero or more `is_final=false` results (the interim results).
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<StreamingRecognitionResult*> *resultsArray;
/** The number of items in @c resultsArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger resultsArray_Count;

/** Output only. Indicates the type of speech event. */
@property(nonatomic, readwrite) StreamingRecognizeResponse_SpeechEventType speechEventType;

@end

/**
 * Fetches the raw value of a @c StreamingRecognizeResponse's @c speechEventType property, even
 * if the value was not defined by the enum at the time the code was generated.
 **/
int32_t StreamingRecognizeResponse_SpeechEventType_RawValue(StreamingRecognizeResponse *message);
/**
 * Sets the raw value of an @c StreamingRecognizeResponse's @c speechEventType property, allowing
 * it to be set to a value that was not defined by the enum at the time the code
 * was generated.
 **/
void SetStreamingRecognizeResponse_SpeechEventType_RawValue(StreamingRecognizeResponse *message, int32_t value);

#pragma mark - StreamingRecognitionResult

typedef GPB_ENUM(StreamingRecognitionResult_FieldNumber) {
  StreamingRecognitionResult_FieldNumber_AlternativesArray = 1,
  StreamingRecognitionResult_FieldNumber_IsFinal = 2,
  StreamingRecognitionResult_FieldNumber_Stability = 3,
  StreamingRecognitionResult_FieldNumber_ResultEndTime = 4,
  StreamingRecognitionResult_FieldNumber_ChannelTag = 5,
  StreamingRecognitionResult_FieldNumber_LanguageCode = 6,
};

/**
 * A streaming speech recognition result corresponding to a portion of the audio
 * that is currently being processed.
 **/
GPB_FINAL @interface StreamingRecognitionResult : GPBMessage

/**
 * Output only. May contain one or more recognition hypotheses (up to the
 * maximum specified in `max_alternatives`).
 * These alternatives are ordered in terms of accuracy, with the top (first)
 * alternative being the most probable, as ranked by the recognizer.
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<SpeechRecognitionAlternative*> *alternativesArray;
/** The number of items in @c alternativesArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger alternativesArray_Count;

/**
 * Output only. If `false`, this `StreamingRecognitionResult` represents an
 * interim result that may change. If `true`, this is the final time the
 * speech service will return this particular `StreamingRecognitionResult`,
 * the recognizer will not return any further hypotheses for this portion of
 * the transcript and corresponding audio.
 **/
@property(nonatomic, readwrite) BOOL isFinal;

/**
 * Output only. An estimate of the likelihood that the recognizer will not
 * change its guess about this interim result. Values range from 0.0
 * (completely unstable) to 1.0 (completely stable).
 * This field is only provided for interim results (`is_final=false`).
 * The default of 0.0 is a sentinel value indicating `stability` was not set.
 **/
@property(nonatomic, readwrite) float stability;

/**
 * Output only. Time offset of the end of this result relative to the
 * beginning of the audio.
 **/
@property(nonatomic, readwrite, strong, null_resettable) GPBDuration *resultEndTime;
/** Test to see if @c resultEndTime has been set. */
@property(nonatomic, readwrite) BOOL hasResultEndTime;

/**
 * For multi-channel audio, this is the channel number corresponding to the
 * recognized result for the audio from that channel.
 * For audio_channel_count = N, its output values can range from '1' to 'N'.
 **/
@property(nonatomic, readwrite) int32_t channelTag;

/**
 * Output only. The
 * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of the
 * language in this result. This language code was detected to have the most
 * likelihood of being spoken in the audio.
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *languageCode;

@end

#pragma mark - SpeechRecognitionResult

typedef GPB_ENUM(SpeechRecognitionResult_FieldNumber) {
  SpeechRecognitionResult_FieldNumber_AlternativesArray = 1,
  SpeechRecognitionResult_FieldNumber_ChannelTag = 2,
  SpeechRecognitionResult_FieldNumber_LanguageCode = 5,
};

/**
 * A speech recognition result corresponding to a portion of the audio.
 **/
GPB_FINAL @interface SpeechRecognitionResult : GPBMessage

/**
 * Output only. May contain one or more recognition hypotheses (up to the
 * maximum specified in `max_alternatives`).
 * These alternatives are ordered in terms of accuracy, with the top (first)
 * alternative being the most probable, as ranked by the recognizer.
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<SpeechRecognitionAlternative*> *alternativesArray;
/** The number of items in @c alternativesArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger alternativesArray_Count;

/**
 * For multi-channel audio, this is the channel number corresponding to the
 * recognized result for the audio from that channel.
 * For audio_channel_count = N, its output values can range from '1' to 'N'.
 **/
@property(nonatomic, readwrite) int32_t channelTag;

/**
 * Output only. The
 * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of the
 * language in this result. This language code was detected to have the most
 * likelihood of being spoken in the audio.
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *languageCode;

@end

#pragma mark - SpeechRecognitionAlternative

typedef GPB_ENUM(SpeechRecognitionAlternative_FieldNumber) {
  SpeechRecognitionAlternative_FieldNumber_Transcript = 1,
  SpeechRecognitionAlternative_FieldNumber_Confidence = 2,
  SpeechRecognitionAlternative_FieldNumber_WordsArray = 3,
};

/**
 * Alternative hypotheses (a.k.a. n-best list).
 **/
GPB_FINAL @interface SpeechRecognitionAlternative : GPBMessage

/** Output only. Transcript text representing the words that the user spoke. */
@property(nonatomic, readwrite, copy, null_resettable) NSString *transcript;

/**
 * Output only. The confidence estimate between 0.0 and 1.0. A higher number
 * indicates an estimated greater likelihood that the recognized words are
 * correct. This field is set only for the top alternative of a non-streaming
 * result or, of a streaming result where `is_final=true`.
 * This field is not guaranteed to be accurate and users should not rely on it
 * to be always provided.
 * The default of 0.0 is a sentinel value indicating `confidence` was not set.
 **/
@property(nonatomic, readwrite) float confidence;

/**
 * Output only. A list of word-specific information for each recognized word.
 * Note: When `enable_speaker_diarization` is true, you will see all the words
 * from the beginning of the audio.
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<WordInfo*> *wordsArray;
/** The number of items in @c wordsArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger wordsArray_Count;

@end

#pragma mark - WordInfo

typedef GPB_ENUM(WordInfo_FieldNumber) {
  WordInfo_FieldNumber_StartTime = 1,
  WordInfo_FieldNumber_EndTime = 2,
  WordInfo_FieldNumber_Word = 3,
  WordInfo_FieldNumber_Confidence = 4,
  WordInfo_FieldNumber_SpeakerTag = 5,
};

/**
 * Word-specific information for recognized words.
 **/
GPB_FINAL @interface WordInfo : GPBMessage

/**
 * Output only. Time offset relative to the beginning of the audio,
 * and corresponding to the start of the spoken word.
 * This field is only set if `enable_word_time_offsets=true` and only
 * in the top hypothesis.
 * This is an experimental feature and the accuracy of the time offset can
 * vary.
 **/
@property(nonatomic, readwrite, strong, null_resettable) GPBDuration *startTime;
/** Test to see if @c startTime has been set. */
@property(nonatomic, readwrite) BOOL hasStartTime;

/**
 * Output only. Time offset relative to the beginning of the audio,
 * and corresponding to the end of the spoken word.
 * This field is only set if `enable_word_time_offsets=true` and only
 * in the top hypothesis.
 * This is an experimental feature and the accuracy of the time offset can
 * vary.
 **/
@property(nonatomic, readwrite, strong, null_resettable) GPBDuration *endTime;
/** Test to see if @c endTime has been set. */
@property(nonatomic, readwrite) BOOL hasEndTime;

/** Output only. The word corresponding to this set of information. */
@property(nonatomic, readwrite, copy, null_resettable) NSString *word;

/**
 * Output only. The confidence estimate between 0.0 and 1.0. A higher number
 * indicates an estimated greater likelihood that the recognized words are
 * correct. This field is set only for the top alternative of a non-streaming
 * result or, of a streaming result where `is_final=true`.
 * This field is not guaranteed to be accurate and users should not rely on it
 * to be always provided.
 * The default of 0.0 is a sentinel value indicating `confidence` was not set.
 **/
@property(nonatomic, readwrite) float confidence;

/**
 * Output only. A distinct integer value is assigned for every speaker within
 * the audio. This field specifies which one of those speakers was detected to
 * have spoken this word. Value ranges from '1' to diarization_speaker_count.
 * speaker_tag is set if enable_speaker_diarization = 'true' and only in the
 * top alternative.
 **/
@property(nonatomic, readwrite) int32_t speakerTag;

@end

NS_ASSUME_NONNULL_END

CF_EXTERN_C_END

#pragma clang diagnostic pop

// @@protoc_insertion_point(global_scope)
