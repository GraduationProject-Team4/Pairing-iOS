// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/cloud/speech/v1p1beta1/cloud_speech.proto

// This CPP symbol can be defined to use imports that match up to the framework
// imports needed when using CocoaPods.
#if !defined(GPB_USE_PROTOBUF_FRAMEWORK_IMPORTS)
 #define GPB_USE_PROTOBUF_FRAMEWORK_IMPORTS 0
#endif

#if GPB_USE_PROTOBUF_FRAMEWORK_IMPORTS
 #import <Protobuf/GPBProtocolBuffers_RuntimeSupport.h>
#else
 #import "GPBProtocolBuffers_RuntimeSupport.h"
#endif

#import <stdatomic.h>

#import "google/cloud/speech/v1p1beta1/CloudSpeech.pbobjc.h"
#import "google/api/Annotations.pbobjc.h"
#import "google/longrunning/Operations.pbobjc.h"
#import "google/rpc/Status.pbobjc.h"
// @@protoc_insertion_point(imports)

#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdeprecated-declarations"
#pragma clang diagnostic ignored "-Wdirect-ivar-access"
#pragma clang diagnostic ignored "-Wdollar-in-identifier-extension"

#pragma mark - Objective C Class declarations
// Forward declarations of Objective C classes that we can use as
// static values in struct initializers.
// We don't use [Foo class] because it is not a static value.
GPBObjCClassDeclaration(GPBDuration);
GPBObjCClassDeclaration(GPBTimestamp);
GPBObjCClassDeclaration(RecognitionAudio);
GPBObjCClassDeclaration(RecognitionConfig);
GPBObjCClassDeclaration(RecognitionMetadata);
GPBObjCClassDeclaration(SpeechContext);
GPBObjCClassDeclaration(SpeechRecognitionAlternative);
GPBObjCClassDeclaration(SpeechRecognitionResult);
GPBObjCClassDeclaration(Status);
GPBObjCClassDeclaration(StreamingRecognitionConfig);
GPBObjCClassDeclaration(StreamingRecognitionResult);
GPBObjCClassDeclaration(WordInfo);

#pragma mark - CloudSpeechRoot

@implementation CloudSpeechRoot

+ (GPBExtensionRegistry*)extensionRegistry {
  // This is called by +initialize so there is no need to worry
  // about thread safety and initialization of registry.
  static GPBExtensionRegistry* registry = nil;
  if (!registry) {
    GPB_DEBUG_CHECK_RUNTIME_VERSIONS();
    registry = [[GPBExtensionRegistry alloc] init];
    // Merge in the imports (direct or indirect) that defined extensions.
    [registry addExtensions:[AnnotationsRoot extensionRegistry]];
  }
  return registry;
}

@end

#pragma mark - CloudSpeechRoot_FileDescriptor

static GPBFileDescriptor *CloudSpeechRoot_FileDescriptor(void) {
  // This is called by +initialize so there is no need to worry
  // about thread safety of the singleton.
  static GPBFileDescriptor *descriptor = NULL;
  if (!descriptor) {
    GPB_DEBUG_CHECK_RUNTIME_VERSIONS();
    descriptor = [[GPBFileDescriptor alloc] initWithPackage:@"google.cloud.speech.v1p1beta1"
                                                     syntax:GPBFileSyntaxProto3];
  }
  return descriptor;
}

#pragma mark - RecognizeRequest

@implementation RecognizeRequest

@dynamic hasConfig, config;
@dynamic hasAudio, audio;

typedef struct RecognizeRequest__storage_ {
  uint32_t _has_storage_[1];
  RecognitionConfig *config;
  RecognitionAudio *audio;
} RecognizeRequest__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "config",
        .dataTypeSpecific.clazz = GPBObjCClass(RecognitionConfig),
        .number = RecognizeRequest_FieldNumber_Config,
        .hasIndex = 0,
        .offset = (uint32_t)offsetof(RecognizeRequest__storage_, config),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "audio",
        .dataTypeSpecific.clazz = GPBObjCClass(RecognitionAudio),
        .number = RecognizeRequest_FieldNumber_Audio,
        .hasIndex = 1,
        .offset = (uint32_t)offsetof(RecognizeRequest__storage_, audio),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[RecognizeRequest class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(RecognizeRequest__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

#pragma mark - LongRunningRecognizeRequest

@implementation LongRunningRecognizeRequest

@dynamic hasConfig, config;
@dynamic hasAudio, audio;

typedef struct LongRunningRecognizeRequest__storage_ {
  uint32_t _has_storage_[1];
  RecognitionConfig *config;
  RecognitionAudio *audio;
} LongRunningRecognizeRequest__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "config",
        .dataTypeSpecific.clazz = GPBObjCClass(RecognitionConfig),
        .number = LongRunningRecognizeRequest_FieldNumber_Config,
        .hasIndex = 0,
        .offset = (uint32_t)offsetof(LongRunningRecognizeRequest__storage_, config),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "audio",
        .dataTypeSpecific.clazz = GPBObjCClass(RecognitionAudio),
        .number = LongRunningRecognizeRequest_FieldNumber_Audio,
        .hasIndex = 1,
        .offset = (uint32_t)offsetof(LongRunningRecognizeRequest__storage_, audio),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[LongRunningRecognizeRequest class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(LongRunningRecognizeRequest__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

#pragma mark - StreamingRecognizeRequest

@implementation StreamingRecognizeRequest

@dynamic streamingRequestOneOfCase;
@dynamic streamingConfig;
@dynamic audioContent;

typedef struct StreamingRecognizeRequest__storage_ {
  uint32_t _has_storage_[2];
  StreamingRecognitionConfig *streamingConfig;
  NSData *audioContent;
} StreamingRecognizeRequest__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "streamingConfig",
        .dataTypeSpecific.clazz = GPBObjCClass(StreamingRecognitionConfig),
        .number = StreamingRecognizeRequest_FieldNumber_StreamingConfig,
        .hasIndex = -1,
        .offset = (uint32_t)offsetof(StreamingRecognizeRequest__storage_, streamingConfig),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "audioContent",
        .dataTypeSpecific.clazz = Nil,
        .number = StreamingRecognizeRequest_FieldNumber_AudioContent,
        .hasIndex = -1,
        .offset = (uint32_t)offsetof(StreamingRecognizeRequest__storage_, audioContent),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeBytes,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[StreamingRecognizeRequest class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(StreamingRecognizeRequest__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    static const char *oneofs[] = {
      "streamingRequest",
    };
    [localDescriptor setupOneofs:oneofs
                           count:(uint32_t)(sizeof(oneofs) / sizeof(char*))
                   firstHasIndex:-1];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

void StreamingRecognizeRequest_ClearStreamingRequestOneOfCase(StreamingRecognizeRequest *message) {
  GPBDescriptor *descriptor = [StreamingRecognizeRequest descriptor];
  GPBOneofDescriptor *oneof = [descriptor.oneofs objectAtIndex:0];
  GPBClearOneof(message, oneof);
}
#pragma mark - StreamingRecognitionConfig

@implementation StreamingRecognitionConfig

@dynamic hasConfig, config;
@dynamic singleUtterance;
@dynamic interimResults;

typedef struct StreamingRecognitionConfig__storage_ {
  uint32_t _has_storage_[1];
  RecognitionConfig *config;
} StreamingRecognitionConfig__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "config",
        .dataTypeSpecific.clazz = GPBObjCClass(RecognitionConfig),
        .number = StreamingRecognitionConfig_FieldNumber_Config,
        .hasIndex = 0,
        .offset = (uint32_t)offsetof(StreamingRecognitionConfig__storage_, config),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "singleUtterance",
        .dataTypeSpecific.clazz = Nil,
        .number = StreamingRecognitionConfig_FieldNumber_SingleUtterance,
        .hasIndex = 1,
        .offset = 2,  // Stored in _has_storage_ to save space.
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeBool,
      },
      {
        .name = "interimResults",
        .dataTypeSpecific.clazz = Nil,
        .number = StreamingRecognitionConfig_FieldNumber_InterimResults,
        .hasIndex = 3,
        .offset = 4,  // Stored in _has_storage_ to save space.
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeBool,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[StreamingRecognitionConfig class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(StreamingRecognitionConfig__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

#pragma mark - RecognitionConfig

@implementation RecognitionConfig

@dynamic encoding;
@dynamic sampleRateHertz;
@dynamic audioChannelCount;
@dynamic enableSeparateRecognitionPerChannel;
@dynamic languageCode;
@dynamic alternativeLanguageCodesArray, alternativeLanguageCodesArray_Count;
@dynamic maxAlternatives;
@dynamic profanityFilter;
@dynamic speechContextsArray, speechContextsArray_Count;
@dynamic enableWordTimeOffsets;
@dynamic enableWordConfidence;
@dynamic enableAutomaticPunctuation;
@dynamic enableSpeakerDiarization;
@dynamic diarizationSpeakerCount;
@dynamic hasMetadata, metadata;
@dynamic model;
@dynamic useEnhanced;

typedef struct RecognitionConfig__storage_ {
  uint32_t _has_storage_[1];
  RecognitionConfig_AudioEncoding encoding;
  int32_t sampleRateHertz;
  int32_t maxAlternatives;
  int32_t audioChannelCount;
  int32_t diarizationSpeakerCount;
  NSString *languageCode;
  NSMutableArray *speechContextsArray;
  RecognitionMetadata *metadata;
  NSString *model;
  NSMutableArray *alternativeLanguageCodesArray;
} RecognitionConfig__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "encoding",
        .dataTypeSpecific.enumDescFunc = RecognitionConfig_AudioEncoding_EnumDescriptor,
        .number = RecognitionConfig_FieldNumber_Encoding,
        .hasIndex = 0,
        .offset = (uint32_t)offsetof(RecognitionConfig__storage_, encoding),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldHasEnumDescriptor | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeEnum,
      },
      {
        .name = "sampleRateHertz",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_SampleRateHertz,
        .hasIndex = 1,
        .offset = (uint32_t)offsetof(RecognitionConfig__storage_, sampleRateHertz),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeInt32,
      },
      {
        .name = "languageCode",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_LanguageCode,
        .hasIndex = 5,
        .offset = (uint32_t)offsetof(RecognitionConfig__storage_, languageCode),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeString,
      },
      {
        .name = "maxAlternatives",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_MaxAlternatives,
        .hasIndex = 6,
        .offset = (uint32_t)offsetof(RecognitionConfig__storage_, maxAlternatives),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeInt32,
      },
      {
        .name = "profanityFilter",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_ProfanityFilter,
        .hasIndex = 7,
        .offset = 8,  // Stored in _has_storage_ to save space.
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeBool,
      },
      {
        .name = "speechContextsArray",
        .dataTypeSpecific.clazz = GPBObjCClass(SpeechContext),
        .number = RecognitionConfig_FieldNumber_SpeechContextsArray,
        .hasIndex = GPBNoHasBit,
        .offset = (uint32_t)offsetof(RecognitionConfig__storage_, speechContextsArray),
        .flags = GPBFieldRepeated,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "audioChannelCount",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_AudioChannelCount,
        .hasIndex = 2,
        .offset = (uint32_t)offsetof(RecognitionConfig__storage_, audioChannelCount),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeInt32,
      },
      {
        .name = "enableWordTimeOffsets",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_EnableWordTimeOffsets,
        .hasIndex = 9,
        .offset = 10,  // Stored in _has_storage_ to save space.
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeBool,
      },
      {
        .name = "metadata",
        .dataTypeSpecific.clazz = GPBObjCClass(RecognitionMetadata),
        .number = RecognitionConfig_FieldNumber_Metadata,
        .hasIndex = 18,
        .offset = (uint32_t)offsetof(RecognitionConfig__storage_, metadata),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "enableAutomaticPunctuation",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_EnableAutomaticPunctuation,
        .hasIndex = 13,
        .offset = 14,  // Stored in _has_storage_ to save space.
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeBool,
      },
      {
        .name = "enableSeparateRecognitionPerChannel",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_EnableSeparateRecognitionPerChannel,
        .hasIndex = 3,
        .offset = 4,  // Stored in _has_storage_ to save space.
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeBool,
      },
      {
        .name = "model",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_Model,
        .hasIndex = 19,
        .offset = (uint32_t)offsetof(RecognitionConfig__storage_, model),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeString,
      },
      {
        .name = "useEnhanced",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_UseEnhanced,
        .hasIndex = 20,
        .offset = 21,  // Stored in _has_storage_ to save space.
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeBool,
      },
      {
        .name = "enableWordConfidence",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_EnableWordConfidence,
        .hasIndex = 11,
        .offset = 12,  // Stored in _has_storage_ to save space.
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeBool,
      },
      {
        .name = "enableSpeakerDiarization",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_EnableSpeakerDiarization,
        .hasIndex = 15,
        .offset = 16,  // Stored in _has_storage_ to save space.
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeBool,
      },
      {
        .name = "diarizationSpeakerCount",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_DiarizationSpeakerCount,
        .hasIndex = 17,
        .offset = (uint32_t)offsetof(RecognitionConfig__storage_, diarizationSpeakerCount),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeInt32,
      },
      {
        .name = "alternativeLanguageCodesArray",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionConfig_FieldNumber_AlternativeLanguageCodesArray,
        .hasIndex = GPBNoHasBit,
        .offset = (uint32_t)offsetof(RecognitionConfig__storage_, alternativeLanguageCodesArray),
        .flags = GPBFieldRepeated,
        .dataType = GPBDataTypeString,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[RecognitionConfig class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(RecognitionConfig__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

int32_t RecognitionConfig_Encoding_RawValue(RecognitionConfig *message) {
  GPBDescriptor *descriptor = [RecognitionConfig descriptor];
  GPBFieldDescriptor *field = [descriptor fieldWithNumber:RecognitionConfig_FieldNumber_Encoding];
  return GPBGetMessageRawEnumField(message, field);
}

void SetRecognitionConfig_Encoding_RawValue(RecognitionConfig *message, int32_t value) {
  GPBDescriptor *descriptor = [RecognitionConfig descriptor];
  GPBFieldDescriptor *field = [descriptor fieldWithNumber:RecognitionConfig_FieldNumber_Encoding];
  GPBSetMessageRawEnumField(message, field, value);
}

#pragma mark - Enum RecognitionConfig_AudioEncoding

GPBEnumDescriptor *RecognitionConfig_AudioEncoding_EnumDescriptor(void) {
  static _Atomic(GPBEnumDescriptor*) descriptor = nil;
  if (!descriptor) {
    static const char *valueNames =
        "EncodingUnspecified\000Linear16\000Flac\000Mulaw\000"
        "Amr\000AmrWb\000OggOpus\000SpeexWithHeaderByte\000";
    static const int32_t values[] = {
        RecognitionConfig_AudioEncoding_EncodingUnspecified,
        RecognitionConfig_AudioEncoding_Linear16,
        RecognitionConfig_AudioEncoding_Flac,
        RecognitionConfig_AudioEncoding_Mulaw,
        RecognitionConfig_AudioEncoding_Amr,
        RecognitionConfig_AudioEncoding_AmrWb,
        RecognitionConfig_AudioEncoding_OggOpus,
        RecognitionConfig_AudioEncoding_SpeexWithHeaderByte,
    };
    GPBEnumDescriptor *worker =
        [GPBEnumDescriptor allocDescriptorForName:GPBNSStringifySymbol(RecognitionConfig_AudioEncoding)
                                       valueNames:valueNames
                                           values:values
                                            count:(uint32_t)(sizeof(values) / sizeof(int32_t))
                                     enumVerifier:RecognitionConfig_AudioEncoding_IsValidValue];
    GPBEnumDescriptor *expected = nil;
    if (!atomic_compare_exchange_strong(&descriptor, &expected, worker)) {
      [worker release];
    }
  }
  return descriptor;
}

BOOL RecognitionConfig_AudioEncoding_IsValidValue(int32_t value__) {
  switch (value__) {
    case RecognitionConfig_AudioEncoding_EncodingUnspecified:
    case RecognitionConfig_AudioEncoding_Linear16:
    case RecognitionConfig_AudioEncoding_Flac:
    case RecognitionConfig_AudioEncoding_Mulaw:
    case RecognitionConfig_AudioEncoding_Amr:
    case RecognitionConfig_AudioEncoding_AmrWb:
    case RecognitionConfig_AudioEncoding_OggOpus:
    case RecognitionConfig_AudioEncoding_SpeexWithHeaderByte:
      return YES;
    default:
      return NO;
  }
}

#pragma mark - RecognitionMetadata

@implementation RecognitionMetadata

@dynamic interactionType;
@dynamic industryNaicsCodeOfAudio;
@dynamic microphoneDistance;
@dynamic originalMediaType;
@dynamic recordingDeviceType;
@dynamic recordingDeviceName;
@dynamic originalMimeType;
@dynamic obfuscatedId;
@dynamic audioTopic;

typedef struct RecognitionMetadata__storage_ {
  uint32_t _has_storage_[1];
  RecognitionMetadata_InteractionType interactionType;
  uint32_t industryNaicsCodeOfAudio;
  RecognitionMetadata_MicrophoneDistance microphoneDistance;
  RecognitionMetadata_OriginalMediaType originalMediaType;
  RecognitionMetadata_RecordingDeviceType recordingDeviceType;
  NSString *recordingDeviceName;
  NSString *originalMimeType;
  NSString *audioTopic;
  int64_t obfuscatedId;
} RecognitionMetadata__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "interactionType",
        .dataTypeSpecific.enumDescFunc = RecognitionMetadata_InteractionType_EnumDescriptor,
        .number = RecognitionMetadata_FieldNumber_InteractionType,
        .hasIndex = 0,
        .offset = (uint32_t)offsetof(RecognitionMetadata__storage_, interactionType),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldHasEnumDescriptor | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeEnum,
      },
      {
        .name = "industryNaicsCodeOfAudio",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionMetadata_FieldNumber_IndustryNaicsCodeOfAudio,
        .hasIndex = 1,
        .offset = (uint32_t)offsetof(RecognitionMetadata__storage_, industryNaicsCodeOfAudio),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeUInt32,
      },
      {
        .name = "microphoneDistance",
        .dataTypeSpecific.enumDescFunc = RecognitionMetadata_MicrophoneDistance_EnumDescriptor,
        .number = RecognitionMetadata_FieldNumber_MicrophoneDistance,
        .hasIndex = 2,
        .offset = (uint32_t)offsetof(RecognitionMetadata__storage_, microphoneDistance),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldHasEnumDescriptor | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeEnum,
      },
      {
        .name = "originalMediaType",
        .dataTypeSpecific.enumDescFunc = RecognitionMetadata_OriginalMediaType_EnumDescriptor,
        .number = RecognitionMetadata_FieldNumber_OriginalMediaType,
        .hasIndex = 3,
        .offset = (uint32_t)offsetof(RecognitionMetadata__storage_, originalMediaType),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldHasEnumDescriptor | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeEnum,
      },
      {
        .name = "recordingDeviceType",
        .dataTypeSpecific.enumDescFunc = RecognitionMetadata_RecordingDeviceType_EnumDescriptor,
        .number = RecognitionMetadata_FieldNumber_RecordingDeviceType,
        .hasIndex = 4,
        .offset = (uint32_t)offsetof(RecognitionMetadata__storage_, recordingDeviceType),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldHasEnumDescriptor | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeEnum,
      },
      {
        .name = "recordingDeviceName",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionMetadata_FieldNumber_RecordingDeviceName,
        .hasIndex = 5,
        .offset = (uint32_t)offsetof(RecognitionMetadata__storage_, recordingDeviceName),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeString,
      },
      {
        .name = "originalMimeType",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionMetadata_FieldNumber_OriginalMimeType,
        .hasIndex = 6,
        .offset = (uint32_t)offsetof(RecognitionMetadata__storage_, originalMimeType),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeString,
      },
      {
        .name = "obfuscatedId",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionMetadata_FieldNumber_ObfuscatedId,
        .hasIndex = 7,
        .offset = (uint32_t)offsetof(RecognitionMetadata__storage_, obfuscatedId),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeInt64,
      },
      {
        .name = "audioTopic",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionMetadata_FieldNumber_AudioTopic,
        .hasIndex = 8,
        .offset = (uint32_t)offsetof(RecognitionMetadata__storage_, audioTopic),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeString,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[RecognitionMetadata class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(RecognitionMetadata__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

int32_t RecognitionMetadata_InteractionType_RawValue(RecognitionMetadata *message) {
  GPBDescriptor *descriptor = [RecognitionMetadata descriptor];
  GPBFieldDescriptor *field = [descriptor fieldWithNumber:RecognitionMetadata_FieldNumber_InteractionType];
  return GPBGetMessageRawEnumField(message, field);
}

void SetRecognitionMetadata_InteractionType_RawValue(RecognitionMetadata *message, int32_t value) {
  GPBDescriptor *descriptor = [RecognitionMetadata descriptor];
  GPBFieldDescriptor *field = [descriptor fieldWithNumber:RecognitionMetadata_FieldNumber_InteractionType];
  GPBSetMessageRawEnumField(message, field, value);
}

int32_t RecognitionMetadata_MicrophoneDistance_RawValue(RecognitionMetadata *message) {
  GPBDescriptor *descriptor = [RecognitionMetadata descriptor];
  GPBFieldDescriptor *field = [descriptor fieldWithNumber:RecognitionMetadata_FieldNumber_MicrophoneDistance];
  return GPBGetMessageRawEnumField(message, field);
}

void SetRecognitionMetadata_MicrophoneDistance_RawValue(RecognitionMetadata *message, int32_t value) {
  GPBDescriptor *descriptor = [RecognitionMetadata descriptor];
  GPBFieldDescriptor *field = [descriptor fieldWithNumber:RecognitionMetadata_FieldNumber_MicrophoneDistance];
  GPBSetMessageRawEnumField(message, field, value);
}

int32_t RecognitionMetadata_OriginalMediaType_RawValue(RecognitionMetadata *message) {
  GPBDescriptor *descriptor = [RecognitionMetadata descriptor];
  GPBFieldDescriptor *field = [descriptor fieldWithNumber:RecognitionMetadata_FieldNumber_OriginalMediaType];
  return GPBGetMessageRawEnumField(message, field);
}

void SetRecognitionMetadata_OriginalMediaType_RawValue(RecognitionMetadata *message, int32_t value) {
  GPBDescriptor *descriptor = [RecognitionMetadata descriptor];
  GPBFieldDescriptor *field = [descriptor fieldWithNumber:RecognitionMetadata_FieldNumber_OriginalMediaType];
  GPBSetMessageRawEnumField(message, field, value);
}

int32_t RecognitionMetadata_RecordingDeviceType_RawValue(RecognitionMetadata *message) {
  GPBDescriptor *descriptor = [RecognitionMetadata descriptor];
  GPBFieldDescriptor *field = [descriptor fieldWithNumber:RecognitionMetadata_FieldNumber_RecordingDeviceType];
  return GPBGetMessageRawEnumField(message, field);
}

void SetRecognitionMetadata_RecordingDeviceType_RawValue(RecognitionMetadata *message, int32_t value) {
  GPBDescriptor *descriptor = [RecognitionMetadata descriptor];
  GPBFieldDescriptor *field = [descriptor fieldWithNumber:RecognitionMetadata_FieldNumber_RecordingDeviceType];
  GPBSetMessageRawEnumField(message, field, value);
}

#pragma mark - Enum RecognitionMetadata_InteractionType

GPBEnumDescriptor *RecognitionMetadata_InteractionType_EnumDescriptor(void) {
  static _Atomic(GPBEnumDescriptor*) descriptor = nil;
  if (!descriptor) {
    static const char *valueNames =
        "InteractionTypeUnspecified\000Discussion\000Pr"
        "esentation\000PhoneCall\000Voicemail\000Professio"
        "nallyProduced\000VoiceSearch\000VoiceCommand\000D"
        "ictation\000";
    static const int32_t values[] = {
        RecognitionMetadata_InteractionType_InteractionTypeUnspecified,
        RecognitionMetadata_InteractionType_Discussion,
        RecognitionMetadata_InteractionType_Presentation,
        RecognitionMetadata_InteractionType_PhoneCall,
        RecognitionMetadata_InteractionType_Voicemail,
        RecognitionMetadata_InteractionType_ProfessionallyProduced,
        RecognitionMetadata_InteractionType_VoiceSearch,
        RecognitionMetadata_InteractionType_VoiceCommand,
        RecognitionMetadata_InteractionType_Dictation,
    };
    GPBEnumDescriptor *worker =
        [GPBEnumDescriptor allocDescriptorForName:GPBNSStringifySymbol(RecognitionMetadata_InteractionType)
                                       valueNames:valueNames
                                           values:values
                                            count:(uint32_t)(sizeof(values) / sizeof(int32_t))
                                     enumVerifier:RecognitionMetadata_InteractionType_IsValidValue];
    GPBEnumDescriptor *expected = nil;
    if (!atomic_compare_exchange_strong(&descriptor, &expected, worker)) {
      [worker release];
    }
  }
  return descriptor;
}

BOOL RecognitionMetadata_InteractionType_IsValidValue(int32_t value__) {
  switch (value__) {
    case RecognitionMetadata_InteractionType_InteractionTypeUnspecified:
    case RecognitionMetadata_InteractionType_Discussion:
    case RecognitionMetadata_InteractionType_Presentation:
    case RecognitionMetadata_InteractionType_PhoneCall:
    case RecognitionMetadata_InteractionType_Voicemail:
    case RecognitionMetadata_InteractionType_ProfessionallyProduced:
    case RecognitionMetadata_InteractionType_VoiceSearch:
    case RecognitionMetadata_InteractionType_VoiceCommand:
    case RecognitionMetadata_InteractionType_Dictation:
      return YES;
    default:
      return NO;
  }
}

#pragma mark - Enum RecognitionMetadata_MicrophoneDistance

GPBEnumDescriptor *RecognitionMetadata_MicrophoneDistance_EnumDescriptor(void) {
  static _Atomic(GPBEnumDescriptor*) descriptor = nil;
  if (!descriptor) {
    static const char *valueNames =
        "MicrophoneDistanceUnspecified\000Nearfield\000"
        "Midfield\000Farfield\000";
    static const int32_t values[] = {
        RecognitionMetadata_MicrophoneDistance_MicrophoneDistanceUnspecified,
        RecognitionMetadata_MicrophoneDistance_Nearfield,
        RecognitionMetadata_MicrophoneDistance_Midfield,
        RecognitionMetadata_MicrophoneDistance_Farfield,
    };
    GPBEnumDescriptor *worker =
        [GPBEnumDescriptor allocDescriptorForName:GPBNSStringifySymbol(RecognitionMetadata_MicrophoneDistance)
                                       valueNames:valueNames
                                           values:values
                                            count:(uint32_t)(sizeof(values) / sizeof(int32_t))
                                     enumVerifier:RecognitionMetadata_MicrophoneDistance_IsValidValue];
    GPBEnumDescriptor *expected = nil;
    if (!atomic_compare_exchange_strong(&descriptor, &expected, worker)) {
      [worker release];
    }
  }
  return descriptor;
}

BOOL RecognitionMetadata_MicrophoneDistance_IsValidValue(int32_t value__) {
  switch (value__) {
    case RecognitionMetadata_MicrophoneDistance_MicrophoneDistanceUnspecified:
    case RecognitionMetadata_MicrophoneDistance_Nearfield:
    case RecognitionMetadata_MicrophoneDistance_Midfield:
    case RecognitionMetadata_MicrophoneDistance_Farfield:
      return YES;
    default:
      return NO;
  }
}

#pragma mark - Enum RecognitionMetadata_OriginalMediaType

GPBEnumDescriptor *RecognitionMetadata_OriginalMediaType_EnumDescriptor(void) {
  static _Atomic(GPBEnumDescriptor*) descriptor = nil;
  if (!descriptor) {
    static const char *valueNames =
        "OriginalMediaTypeUnspecified\000Audio\000Video"
        "\000";
    static const int32_t values[] = {
        RecognitionMetadata_OriginalMediaType_OriginalMediaTypeUnspecified,
        RecognitionMetadata_OriginalMediaType_Audio,
        RecognitionMetadata_OriginalMediaType_Video,
    };
    GPBEnumDescriptor *worker =
        [GPBEnumDescriptor allocDescriptorForName:GPBNSStringifySymbol(RecognitionMetadata_OriginalMediaType)
                                       valueNames:valueNames
                                           values:values
                                            count:(uint32_t)(sizeof(values) / sizeof(int32_t))
                                     enumVerifier:RecognitionMetadata_OriginalMediaType_IsValidValue];
    GPBEnumDescriptor *expected = nil;
    if (!atomic_compare_exchange_strong(&descriptor, &expected, worker)) {
      [worker release];
    }
  }
  return descriptor;
}

BOOL RecognitionMetadata_OriginalMediaType_IsValidValue(int32_t value__) {
  switch (value__) {
    case RecognitionMetadata_OriginalMediaType_OriginalMediaTypeUnspecified:
    case RecognitionMetadata_OriginalMediaType_Audio:
    case RecognitionMetadata_OriginalMediaType_Video:
      return YES;
    default:
      return NO;
  }
}

#pragma mark - Enum RecognitionMetadata_RecordingDeviceType

GPBEnumDescriptor *RecognitionMetadata_RecordingDeviceType_EnumDescriptor(void) {
  static _Atomic(GPBEnumDescriptor*) descriptor = nil;
  if (!descriptor) {
    static const char *valueNames =
        "RecordingDeviceTypeUnspecified\000Smartphon"
        "e\000Pc\000PhoneLine\000Vehicle\000OtherOutdoorDevic"
        "e\000OtherIndoorDevice\000";
    static const int32_t values[] = {
        RecognitionMetadata_RecordingDeviceType_RecordingDeviceTypeUnspecified,
        RecognitionMetadata_RecordingDeviceType_Smartphone,
        RecognitionMetadata_RecordingDeviceType_Pc,
        RecognitionMetadata_RecordingDeviceType_PhoneLine,
        RecognitionMetadata_RecordingDeviceType_Vehicle,
        RecognitionMetadata_RecordingDeviceType_OtherOutdoorDevice,
        RecognitionMetadata_RecordingDeviceType_OtherIndoorDevice,
    };
    GPBEnumDescriptor *worker =
        [GPBEnumDescriptor allocDescriptorForName:GPBNSStringifySymbol(RecognitionMetadata_RecordingDeviceType)
                                       valueNames:valueNames
                                           values:values
                                            count:(uint32_t)(sizeof(values) / sizeof(int32_t))
                                     enumVerifier:RecognitionMetadata_RecordingDeviceType_IsValidValue];
    GPBEnumDescriptor *expected = nil;
    if (!atomic_compare_exchange_strong(&descriptor, &expected, worker)) {
      [worker release];
    }
  }
  return descriptor;
}

BOOL RecognitionMetadata_RecordingDeviceType_IsValidValue(int32_t value__) {
  switch (value__) {
    case RecognitionMetadata_RecordingDeviceType_RecordingDeviceTypeUnspecified:
    case RecognitionMetadata_RecordingDeviceType_Smartphone:
    case RecognitionMetadata_RecordingDeviceType_Pc:
    case RecognitionMetadata_RecordingDeviceType_PhoneLine:
    case RecognitionMetadata_RecordingDeviceType_Vehicle:
    case RecognitionMetadata_RecordingDeviceType_OtherOutdoorDevice:
    case RecognitionMetadata_RecordingDeviceType_OtherIndoorDevice:
      return YES;
    default:
      return NO;
  }
}

#pragma mark - SpeechContext

@implementation SpeechContext

@dynamic phrasesArray, phrasesArray_Count;

typedef struct SpeechContext__storage_ {
  uint32_t _has_storage_[1];
  NSMutableArray *phrasesArray;
} SpeechContext__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "phrasesArray",
        .dataTypeSpecific.clazz = Nil,
        .number = SpeechContext_FieldNumber_PhrasesArray,
        .hasIndex = GPBNoHasBit,
        .offset = (uint32_t)offsetof(SpeechContext__storage_, phrasesArray),
        .flags = GPBFieldRepeated,
        .dataType = GPBDataTypeString,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[SpeechContext class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(SpeechContext__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

#pragma mark - RecognitionAudio

@implementation RecognitionAudio

@dynamic audioSourceOneOfCase;
@dynamic content;
@dynamic uri;

typedef struct RecognitionAudio__storage_ {
  uint32_t _has_storage_[2];
  NSData *content;
  NSString *uri;
} RecognitionAudio__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "content",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionAudio_FieldNumber_Content,
        .hasIndex = -1,
        .offset = (uint32_t)offsetof(RecognitionAudio__storage_, content),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeBytes,
      },
      {
        .name = "uri",
        .dataTypeSpecific.clazz = Nil,
        .number = RecognitionAudio_FieldNumber_Uri,
        .hasIndex = -1,
        .offset = (uint32_t)offsetof(RecognitionAudio__storage_, uri),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeString,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[RecognitionAudio class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(RecognitionAudio__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    static const char *oneofs[] = {
      "audioSource",
    };
    [localDescriptor setupOneofs:oneofs
                           count:(uint32_t)(sizeof(oneofs) / sizeof(char*))
                   firstHasIndex:-1];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

void RecognitionAudio_ClearAudioSourceOneOfCase(RecognitionAudio *message) {
  GPBDescriptor *descriptor = [RecognitionAudio descriptor];
  GPBOneofDescriptor *oneof = [descriptor.oneofs objectAtIndex:0];
  GPBClearOneof(message, oneof);
}
#pragma mark - RecognizeResponse

@implementation RecognizeResponse

@dynamic resultsArray, resultsArray_Count;

typedef struct RecognizeResponse__storage_ {
  uint32_t _has_storage_[1];
  NSMutableArray *resultsArray;
} RecognizeResponse__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "resultsArray",
        .dataTypeSpecific.clazz = GPBObjCClass(SpeechRecognitionResult),
        .number = RecognizeResponse_FieldNumber_ResultsArray,
        .hasIndex = GPBNoHasBit,
        .offset = (uint32_t)offsetof(RecognizeResponse__storage_, resultsArray),
        .flags = GPBFieldRepeated,
        .dataType = GPBDataTypeMessage,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[RecognizeResponse class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(RecognizeResponse__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

#pragma mark - LongRunningRecognizeResponse

@implementation LongRunningRecognizeResponse

@dynamic resultsArray, resultsArray_Count;

typedef struct LongRunningRecognizeResponse__storage_ {
  uint32_t _has_storage_[1];
  NSMutableArray *resultsArray;
} LongRunningRecognizeResponse__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "resultsArray",
        .dataTypeSpecific.clazz = GPBObjCClass(SpeechRecognitionResult),
        .number = LongRunningRecognizeResponse_FieldNumber_ResultsArray,
        .hasIndex = GPBNoHasBit,
        .offset = (uint32_t)offsetof(LongRunningRecognizeResponse__storage_, resultsArray),
        .flags = GPBFieldRepeated,
        .dataType = GPBDataTypeMessage,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[LongRunningRecognizeResponse class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(LongRunningRecognizeResponse__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

#pragma mark - LongRunningRecognizeMetadata

@implementation LongRunningRecognizeMetadata

@dynamic progressPercent;
@dynamic hasStartTime, startTime;
@dynamic hasLastUpdateTime, lastUpdateTime;

typedef struct LongRunningRecognizeMetadata__storage_ {
  uint32_t _has_storage_[1];
  int32_t progressPercent;
  GPBTimestamp *startTime;
  GPBTimestamp *lastUpdateTime;
} LongRunningRecognizeMetadata__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "progressPercent",
        .dataTypeSpecific.clazz = Nil,
        .number = LongRunningRecognizeMetadata_FieldNumber_ProgressPercent,
        .hasIndex = 0,
        .offset = (uint32_t)offsetof(LongRunningRecognizeMetadata__storage_, progressPercent),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeInt32,
      },
      {
        .name = "startTime",
        .dataTypeSpecific.clazz = GPBObjCClass(GPBTimestamp),
        .number = LongRunningRecognizeMetadata_FieldNumber_StartTime,
        .hasIndex = 1,
        .offset = (uint32_t)offsetof(LongRunningRecognizeMetadata__storage_, startTime),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "lastUpdateTime",
        .dataTypeSpecific.clazz = GPBObjCClass(GPBTimestamp),
        .number = LongRunningRecognizeMetadata_FieldNumber_LastUpdateTime,
        .hasIndex = 2,
        .offset = (uint32_t)offsetof(LongRunningRecognizeMetadata__storage_, lastUpdateTime),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[LongRunningRecognizeMetadata class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(LongRunningRecognizeMetadata__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

#pragma mark - StreamingRecognizeResponse

@implementation StreamingRecognizeResponse

@dynamic hasError, error;
@dynamic resultsArray, resultsArray_Count;
@dynamic speechEventType;

typedef struct StreamingRecognizeResponse__storage_ {
  uint32_t _has_storage_[1];
  StreamingRecognizeResponse_SpeechEventType speechEventType;
  Status *error;
  NSMutableArray *resultsArray;
} StreamingRecognizeResponse__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "error",
        .dataTypeSpecific.clazz = GPBObjCClass(Status),
        .number = StreamingRecognizeResponse_FieldNumber_Error,
        .hasIndex = 0,
        .offset = (uint32_t)offsetof(StreamingRecognizeResponse__storage_, error),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "resultsArray",
        .dataTypeSpecific.clazz = GPBObjCClass(StreamingRecognitionResult),
        .number = StreamingRecognizeResponse_FieldNumber_ResultsArray,
        .hasIndex = GPBNoHasBit,
        .offset = (uint32_t)offsetof(StreamingRecognizeResponse__storage_, resultsArray),
        .flags = GPBFieldRepeated,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "speechEventType",
        .dataTypeSpecific.enumDescFunc = StreamingRecognizeResponse_SpeechEventType_EnumDescriptor,
        .number = StreamingRecognizeResponse_FieldNumber_SpeechEventType,
        .hasIndex = 1,
        .offset = (uint32_t)offsetof(StreamingRecognizeResponse__storage_, speechEventType),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldHasEnumDescriptor | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeEnum,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[StreamingRecognizeResponse class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(StreamingRecognizeResponse__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

int32_t StreamingRecognizeResponse_SpeechEventType_RawValue(StreamingRecognizeResponse *message) {
  GPBDescriptor *descriptor = [StreamingRecognizeResponse descriptor];
  GPBFieldDescriptor *field = [descriptor fieldWithNumber:StreamingRecognizeResponse_FieldNumber_SpeechEventType];
  return GPBGetMessageRawEnumField(message, field);
}

void SetStreamingRecognizeResponse_SpeechEventType_RawValue(StreamingRecognizeResponse *message, int32_t value) {
  GPBDescriptor *descriptor = [StreamingRecognizeResponse descriptor];
  GPBFieldDescriptor *field = [descriptor fieldWithNumber:StreamingRecognizeResponse_FieldNumber_SpeechEventType];
  GPBSetMessageRawEnumField(message, field, value);
}

#pragma mark - Enum StreamingRecognizeResponse_SpeechEventType

GPBEnumDescriptor *StreamingRecognizeResponse_SpeechEventType_EnumDescriptor(void) {
  static _Atomic(GPBEnumDescriptor*) descriptor = nil;
  if (!descriptor) {
    static const char *valueNames =
        "SpeechEventUnspecified\000EndOfSingleUttera"
        "nce\000";
    static const int32_t values[] = {
        StreamingRecognizeResponse_SpeechEventType_SpeechEventUnspecified,
        StreamingRecognizeResponse_SpeechEventType_EndOfSingleUtterance,
    };
    GPBEnumDescriptor *worker =
        [GPBEnumDescriptor allocDescriptorForName:GPBNSStringifySymbol(StreamingRecognizeResponse_SpeechEventType)
                                       valueNames:valueNames
                                           values:values
                                            count:(uint32_t)(sizeof(values) / sizeof(int32_t))
                                     enumVerifier:StreamingRecognizeResponse_SpeechEventType_IsValidValue];
    GPBEnumDescriptor *expected = nil;
    if (!atomic_compare_exchange_strong(&descriptor, &expected, worker)) {
      [worker release];
    }
  }
  return descriptor;
}

BOOL StreamingRecognizeResponse_SpeechEventType_IsValidValue(int32_t value__) {
  switch (value__) {
    case StreamingRecognizeResponse_SpeechEventType_SpeechEventUnspecified:
    case StreamingRecognizeResponse_SpeechEventType_EndOfSingleUtterance:
      return YES;
    default:
      return NO;
  }
}

#pragma mark - StreamingRecognitionResult

@implementation StreamingRecognitionResult

@dynamic alternativesArray, alternativesArray_Count;
@dynamic isFinal;
@dynamic stability;
@dynamic hasResultEndTime, resultEndTime;
@dynamic channelTag;
@dynamic languageCode;

typedef struct StreamingRecognitionResult__storage_ {
  uint32_t _has_storage_[1];
  float stability;
  int32_t channelTag;
  NSMutableArray *alternativesArray;
  GPBDuration *resultEndTime;
  NSString *languageCode;
} StreamingRecognitionResult__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "alternativesArray",
        .dataTypeSpecific.clazz = GPBObjCClass(SpeechRecognitionAlternative),
        .number = StreamingRecognitionResult_FieldNumber_AlternativesArray,
        .hasIndex = GPBNoHasBit,
        .offset = (uint32_t)offsetof(StreamingRecognitionResult__storage_, alternativesArray),
        .flags = GPBFieldRepeated,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "isFinal",
        .dataTypeSpecific.clazz = Nil,
        .number = StreamingRecognitionResult_FieldNumber_IsFinal,
        .hasIndex = 0,
        .offset = 1,  // Stored in _has_storage_ to save space.
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeBool,
      },
      {
        .name = "stability",
        .dataTypeSpecific.clazz = Nil,
        .number = StreamingRecognitionResult_FieldNumber_Stability,
        .hasIndex = 2,
        .offset = (uint32_t)offsetof(StreamingRecognitionResult__storage_, stability),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeFloat,
      },
      {
        .name = "resultEndTime",
        .dataTypeSpecific.clazz = GPBObjCClass(GPBDuration),
        .number = StreamingRecognitionResult_FieldNumber_ResultEndTime,
        .hasIndex = 3,
        .offset = (uint32_t)offsetof(StreamingRecognitionResult__storage_, resultEndTime),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "channelTag",
        .dataTypeSpecific.clazz = Nil,
        .number = StreamingRecognitionResult_FieldNumber_ChannelTag,
        .hasIndex = 4,
        .offset = (uint32_t)offsetof(StreamingRecognitionResult__storage_, channelTag),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeInt32,
      },
      {
        .name = "languageCode",
        .dataTypeSpecific.clazz = Nil,
        .number = StreamingRecognitionResult_FieldNumber_LanguageCode,
        .hasIndex = 5,
        .offset = (uint32_t)offsetof(StreamingRecognitionResult__storage_, languageCode),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeString,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[StreamingRecognitionResult class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(StreamingRecognitionResult__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

#pragma mark - SpeechRecognitionResult

@implementation SpeechRecognitionResult

@dynamic alternativesArray, alternativesArray_Count;
@dynamic channelTag;
@dynamic languageCode;

typedef struct SpeechRecognitionResult__storage_ {
  uint32_t _has_storage_[1];
  int32_t channelTag;
  NSMutableArray *alternativesArray;
  NSString *languageCode;
} SpeechRecognitionResult__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "alternativesArray",
        .dataTypeSpecific.clazz = GPBObjCClass(SpeechRecognitionAlternative),
        .number = SpeechRecognitionResult_FieldNumber_AlternativesArray,
        .hasIndex = GPBNoHasBit,
        .offset = (uint32_t)offsetof(SpeechRecognitionResult__storage_, alternativesArray),
        .flags = GPBFieldRepeated,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "channelTag",
        .dataTypeSpecific.clazz = Nil,
        .number = SpeechRecognitionResult_FieldNumber_ChannelTag,
        .hasIndex = 0,
        .offset = (uint32_t)offsetof(SpeechRecognitionResult__storage_, channelTag),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeInt32,
      },
      {
        .name = "languageCode",
        .dataTypeSpecific.clazz = Nil,
        .number = SpeechRecognitionResult_FieldNumber_LanguageCode,
        .hasIndex = 1,
        .offset = (uint32_t)offsetof(SpeechRecognitionResult__storage_, languageCode),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeString,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[SpeechRecognitionResult class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(SpeechRecognitionResult__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

#pragma mark - SpeechRecognitionAlternative

@implementation SpeechRecognitionAlternative

@dynamic transcript;
@dynamic confidence;
@dynamic wordsArray, wordsArray_Count;

typedef struct SpeechRecognitionAlternative__storage_ {
  uint32_t _has_storage_[1];
  float confidence;
  NSString *transcript;
  NSMutableArray *wordsArray;
} SpeechRecognitionAlternative__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "transcript",
        .dataTypeSpecific.clazz = Nil,
        .number = SpeechRecognitionAlternative_FieldNumber_Transcript,
        .hasIndex = 0,
        .offset = (uint32_t)offsetof(SpeechRecognitionAlternative__storage_, transcript),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeString,
      },
      {
        .name = "confidence",
        .dataTypeSpecific.clazz = Nil,
        .number = SpeechRecognitionAlternative_FieldNumber_Confidence,
        .hasIndex = 1,
        .offset = (uint32_t)offsetof(SpeechRecognitionAlternative__storage_, confidence),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeFloat,
      },
      {
        .name = "wordsArray",
        .dataTypeSpecific.clazz = GPBObjCClass(WordInfo),
        .number = SpeechRecognitionAlternative_FieldNumber_WordsArray,
        .hasIndex = GPBNoHasBit,
        .offset = (uint32_t)offsetof(SpeechRecognitionAlternative__storage_, wordsArray),
        .flags = GPBFieldRepeated,
        .dataType = GPBDataTypeMessage,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[SpeechRecognitionAlternative class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(SpeechRecognitionAlternative__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end

#pragma mark - WordInfo

@implementation WordInfo

@dynamic hasStartTime, startTime;
@dynamic hasEndTime, endTime;
@dynamic word;
@dynamic confidence;
@dynamic speakerTag;

typedef struct WordInfo__storage_ {
  uint32_t _has_storage_[1];
  float confidence;
  int32_t speakerTag;
  GPBDuration *startTime;
  GPBDuration *endTime;
  NSString *word;
} WordInfo__storage_;

// This method is threadsafe because it is initially called
// in +initialize for each subclass.
+ (GPBDescriptor *)descriptor {
  static GPBDescriptor *descriptor = nil;
  if (!descriptor) {
    static GPBMessageFieldDescription fields[] = {
      {
        .name = "startTime",
        .dataTypeSpecific.clazz = GPBObjCClass(GPBDuration),
        .number = WordInfo_FieldNumber_StartTime,
        .hasIndex = 0,
        .offset = (uint32_t)offsetof(WordInfo__storage_, startTime),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "endTime",
        .dataTypeSpecific.clazz = GPBObjCClass(GPBDuration),
        .number = WordInfo_FieldNumber_EndTime,
        .hasIndex = 1,
        .offset = (uint32_t)offsetof(WordInfo__storage_, endTime),
        .flags = GPBFieldOptional,
        .dataType = GPBDataTypeMessage,
      },
      {
        .name = "word",
        .dataTypeSpecific.clazz = Nil,
        .number = WordInfo_FieldNumber_Word,
        .hasIndex = 2,
        .offset = (uint32_t)offsetof(WordInfo__storage_, word),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeString,
      },
      {
        .name = "confidence",
        .dataTypeSpecific.clazz = Nil,
        .number = WordInfo_FieldNumber_Confidence,
        .hasIndex = 3,
        .offset = (uint32_t)offsetof(WordInfo__storage_, confidence),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeFloat,
      },
      {
        .name = "speakerTag",
        .dataTypeSpecific.clazz = Nil,
        .number = WordInfo_FieldNumber_SpeakerTag,
        .hasIndex = 4,
        .offset = (uint32_t)offsetof(WordInfo__storage_, speakerTag),
        .flags = (GPBFieldFlags)(GPBFieldOptional | GPBFieldClearHasIvarOnZero),
        .dataType = GPBDataTypeInt32,
      },
    };
    GPBDescriptor *localDescriptor =
        [GPBDescriptor allocDescriptorForClass:[WordInfo class]
                                     rootClass:[CloudSpeechRoot class]
                                          file:CloudSpeechRoot_FileDescriptor()
                                        fields:fields
                                    fieldCount:(uint32_t)(sizeof(fields) / sizeof(GPBMessageFieldDescription))
                                   storageSize:sizeof(WordInfo__storage_)
                                         flags:(GPBDescriptorInitializationFlags)(GPBDescriptorInitializationFlag_UsesClassRefs | GPBDescriptorInitializationFlag_Proto3OptionalKnown)];
    #if defined(DEBUG) && DEBUG
      NSAssert(descriptor == nil, @"Startup recursed!");
    #endif  // DEBUG
    descriptor = localDescriptor;
  }
  return descriptor;
}

@end


#pragma clang diagnostic pop

// @@protoc_insertion_point(global_scope)
